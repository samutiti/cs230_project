{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# CS230 Project - Macrophage Classification\n",
    "\n",
    "classify macrophage cells into 8 different classes using resnet50\n",
    "\n",
    "the 8 classes are: Dividing, Early Phagocytosis, Fried egg, Intermediate Phagocytosis, Late Phagocytosis, Migrating, Quiescent, and Searching\n",
    "\n",
    "We use CellSighter (https://github.com/KerenLab/CellSighter) as the starting point for our classification task. The model is implemented in PyTorch and trained using a ResNet-50 backbone."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import sys\n",
    "sys.path.append(\".\")\n",
    "import os\n",
    "import torch\n",
    "import numpy as np\n",
    "import json\n",
    "import tifffile\n",
    "import cv2\n",
    "from torch.utils.data import Dataset, DataLoader, WeightedRandomSampler\n",
    "from torchvision import models\n",
    "from torch import nn\n",
    "import torchvision\n",
    "from torchvision.transforms import Lambda\n",
    "from torch.utils.tensorboard import SummaryWriter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## data preprocessing\n",
    "\n",
    "first need to preprocess the raw labels and split into train/val sets. \n",
    "\n",
    "**note**: i already ran this part so you can skip these cells if train.json and val.json already exist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "we have 8 classes\n"
     ]
    }
   ],
   "source": [
    "# paths to raw data\n",
    "labels_path = '/scratch/groups/emmalu/samutiti/phage_labels.json'\n",
    "images_dir = '/scratch/groups/emmalu/samutiti/raw_phage_crops'\n",
    "output_dir = '/scratch/users/rchi/cs230/macrophage_data'\n",
    "\n",
    "# class names in alphabetical order\n",
    "class_names = [\n",
    "    'Dividing',\n",
    "    'Early Phagocytosis',\n",
    "    'Fried egg',\n",
    "    'Intermediate Phagocytosis',\n",
    "    'Late Phagocytosis',\n",
    "    'Migrating',\n",
    "    'Quiescent',\n",
    "    'Searching'\n",
    "]\n",
    "\n",
    "print(f\"we have {len(class_names)} classes\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total samples in raw data: 4052\n",
      "after removing uncertain: 2349\n"
     ]
    }
   ],
   "source": [
    "# load labels from json\n",
    "with open(labels_path, 'r') as f:\n",
    "    all_labels = json.load(f)\n",
    "\n",
    "print(f\"total samples in raw data: {len(all_labels)}\")\n",
    "\n",
    "# filter out uncertain samples\n",
    "# some cells are labeled as \"Uncertain\" which we won't use for training\n",
    "filtered_labels = {}\n",
    "for filename, label_list in all_labels.items():\n",
    "    if 'Uncertain' not in label_list:\n",
    "        filtered_labels[filename] = label_list\n",
    "\n",
    "print(f\"after removing uncertain: {len(filtered_labels)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples with multiple labels: 1078\n",
      "total dataset size: 2349\n"
     ]
    }
   ],
   "source": [
    "# handle multi-label samples\n",
    "# some cells have multiple labels like [\"Migrating\", \"Searching\"]\n",
    "# for simplicity just take the first label\n",
    "label2idx = {name: idx for idx, name in enumerate(class_names)}\n",
    "\n",
    "dataset = []\n",
    "multi_label_count = 0\n",
    "\n",
    "for filename, label_list in filtered_labels.items():\n",
    "    if len(label_list) > 1:\n",
    "        multi_label_count += 1\n",
    "    \n",
    "    # take first label as primary\n",
    "    primary_label = label_list[0]\n",
    "    label_idx = label2idx[primary_label]\n",
    "    \n",
    "    # fix filename (remove _V suffix if exists)\n",
    "    actual_filename = filename.replace('_V.tiff', '.tiff')\n",
    "    \n",
    "    dataset.append({\n",
    "        'filename': actual_filename,\n",
    "        'label': label_idx,\n",
    "        'original_labels': label_list\n",
    "    })\n",
    "\n",
    "print(f\"samples with multiple labels: {multi_label_count}\")\n",
    "print(f\"total dataset size: {len(dataset)}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train samples: 1882\n",
      "validation samples: 467\n",
      "\n",
      "train set class distribution:\n",
      "  Dividing: 196\n",
      "  Early Phagocytosis: 75\n",
      "  Fried egg: 432\n",
      "  Intermediate Phagocytosis: 60\n",
      "  Late Phagocytosis: 61\n",
      "  Migrating: 651\n",
      "  Quiescent: 57\n",
      "  Searching: 350\n",
      "\n",
      "val set class distribution:\n",
      "  Dividing: 49\n",
      "  Early Phagocytosis: 18\n",
      "  Fried egg: 107\n",
      "  Intermediate Phagocytosis: 15\n",
      "  Late Phagocytosis: 15\n",
      "  Migrating: 162\n",
      "  Quiescent: 14\n",
      "  Searching: 87\n"
     ]
    }
   ],
   "source": [
    "# split into train and validation sets (80/20 split)\n",
    "# use stratified split to keep class distribution same in both sets\n",
    "np.random.seed(42)\n",
    "\n",
    "# group samples by class\n",
    "class_samples = {}\n",
    "for sample in dataset:\n",
    "    label = sample['label']\n",
    "    if label not in class_samples:\n",
    "        class_samples[label] = []\n",
    "    class_samples[label].append(sample)\n",
    "\n",
    "train_data = []\n",
    "val_data = []\n",
    "\n",
    "# split each class separately to maintain distribution\n",
    "for label, samples in class_samples.items():\n",
    "    np.random.shuffle(samples)\n",
    "    n_val = int(len(samples) * 0.2)  # 20% for validation\n",
    "    \n",
    "    val_data.extend(samples[:n_val])\n",
    "    train_data.extend(samples[n_val:])\n",
    "\n",
    "# shuffle again\n",
    "np.random.shuffle(train_data)\n",
    "np.random.shuffle(val_data)\n",
    "\n",
    "print(f\"train samples: {len(train_data)}\")\n",
    "print(f\"validation samples: {len(val_data)}\")\n",
    "\n",
    "# show class distribution\n",
    "print(\"\\ntrain set class distribution:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    count = sum(1 for s in train_data if s['label'] == i)\n",
    "    print(f\"  {name}: {count}\")\n",
    "\n",
    "print(\"\\nval set class distribution:\")\n",
    "for i, name in enumerate(class_names):\n",
    "    count = sum(1 for s in val_data if s['label'] == i)\n",
    "    print(f\"  {name}: {count}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# save train and val splits to json files\n",
    "os.makedirs(output_dir, exist_ok=True)\n",
    "\n",
    "train_path = os.path.join(output_dir, 'train.json')\n",
    "val_path = os.path.join(output_dir, 'val.json')\n",
    "\n",
    "with open(train_path, 'w') as f:\n",
    "    json.dump(train_data, f, indent=2)\n",
    "\n",
    "with open(val_path, 'w') as f:\n",
    "    json.dump(val_data, f, indent=2)\n",
    "\n",
    "# also save class names for reference\n",
    "with open(os.path.join(output_dir, 'class_names.txt'), 'w') as f:\n",
    "    f.write('\\n'.join(class_names))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training\n",
    "\n",
    "from here we load the preprocessed data and train the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'crop_input_size': 60, 'crop_size': 128, 'root_dir': '/scratch/users/rchi/cs230/macrophage_data', 'train_set': ['train'], 'val_set': ['val'], 'num_classes': 8, 'epoch_max': 50, 'lr': 0.001, 'blacklist': [], 'batch_size': 32, 'num_workers': 4, 'channels_path': '/scratch/users/rchi/cs230/macrophage_data/channels.txt', 'weight_to_eval': '', 'sample_batch': True, 'to_pad': False, 'hierarchy_match': None, 'size_data': None, 'aug': True}\n"
     ]
    }
   ],
   "source": [
    "# load config\n",
    "config_path = '/scratch/users/rchi/cs230/macrophage_experiment/config.json'\n",
    "with open(config_path) as f:\n",
    "    config = json.load(f)\n",
    "\n",
    "print(config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Loader for macrophage crops\n",
    "\n",
    "The macrophage images have already been cropped into single-cell macrophage tiff images\n",
    "\n",
    "data loader was adapted from cell_crop.py in the CellSighter repository"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# class to load one crop image\n",
    "class MacrophageCrop:\n",
    "    def __init__(self, filename, label, images_dir):\n",
    "        self._filename = filename\n",
    "        self._label = label\n",
    "        self._images_dir = images_dir\n",
    "        self._image_id = filename\n",
    "        self._cell_id = 0\n",
    "    \n",
    "    def sample(self, mask=False):\n",
    "        # load the tiff image\n",
    "        img_path = os.path.join(self._images_dir, self._filename)\n",
    "        image = tifffile.imread(img_path).astype(np.float32)\n",
    "        \n",
    "        # make sure its H x W x C\n",
    "        if len(image.shape) == 2:\n",
    "            image = image[:, :, np.newaxis]\n",
    "        \n",
    "        result = {\n",
    "            'cell_id': self._cell_id,\n",
    "            'image_id': self._image_id,\n",
    "            'image': image,\n",
    "            'label': np.array(self._label, dtype=np.longlong),\n",
    "        }\n",
    "        \n",
    "        if mask:\n",
    "            # create mask - since each crop has one cell just use all ones\n",
    "            h, w = image.shape[:2]\n",
    "            result['mask'] = np.ones((h, w), dtype=np.float32)\n",
    "            result['all_cells_mask'] = np.ones((h, w), dtype=np.float32)\n",
    "        \n",
    "        return result\n",
    "\n",
    "# function to load all crops from json\n",
    "def load_crops(data_json_path, images_dir):\n",
    "    with open(data_json_path, 'r') as f:\n",
    "        data = json.load(f)\n",
    "    \n",
    "    crops = []\n",
    "    for item in data:\n",
    "        crop = MacrophageCrop(\n",
    "            filename=item['filename'],\n",
    "            label=item['label'],\n",
    "            images_dir=images_dir\n",
    "        )\n",
    "        crops.append(crop)\n",
    "    \n",
    "    return crops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## augmentation and transforms\n",
    "\n",
    "we did some data augmentation because we only have 2000-3000 labeled crops. \n",
    "\n",
    "Since the crops are not uniform in size, smaller crops were padded to 60Ã—60\n",
    "\n",
    "Most of the augmentation functions were adapted from the CellSighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import shift augmentation from the working file\n",
    "import sys\n",
    "sys.path.insert(0, '/scratch/users/rchi/cs230')\n",
    "from data.shift_augmentation import ShiftAugmentation\n",
    "\n",
    "# poisson sampling augmentation\n",
    "def poisson_sampling(x):\n",
    "    # add some noise using poisson distribution\n",
    "    blur = cv2.GaussianBlur(x[:, :, :-2], (5, 5), 0)\n",
    "    x[:, :, :-2] = np.random.poisson(lam=blur, size=x[:, :, :-2].shape)\n",
    "    return x\n",
    "\n",
    "# augment cell mask shape\n",
    "def cell_shape_aug(x):\n",
    "    if np.random.random() < 0.5:\n",
    "        cell_mask = x[:, :, -1]\n",
    "        kernel_size = np.random.choice([2, 3, 5])\n",
    "        kernel = np.ones(kernel_size, np.uint8)\n",
    "        img_dilation = cv2.dilate(cell_mask, kernel, iterations=1)\n",
    "        x[:, :, -1] = img_dilation\n",
    "    return x\n",
    "\n",
    "# augment environment mask\n",
    "def env_shape_aug(x):\n",
    "    if np.random.random() < 0.5:\n",
    "        cell_mask = x[:, :, -2]\n",
    "        kernel_size = np.random.choice([2, 3, 5])\n",
    "        kernel = np.ones(kernel_size, np.uint8)\n",
    "        img_dilation = cv2.dilate(cell_mask, kernel, iterations=1)\n",
    "        x[:, :, -2] = img_dilation\n",
    "    return x\n",
    "\n",
    "# pad image to target size if its too small\n",
    "def pad_to_size(x, target_size):\n",
    "    c, h, w = x.shape\n",
    "    pad_h = max(0, target_size - h)\n",
    "    pad_w = max(0, target_size - w)\n",
    "    \n",
    "    if pad_h > 0 or pad_w > 0:\n",
    "        pad_left = pad_w // 2\n",
    "        pad_right = pad_w - pad_left\n",
    "        pad_top = pad_h // 2\n",
    "        pad_bottom = pad_h - pad_top\n",
    "        \n",
    "        padding = (pad_left, pad_right, pad_top, pad_bottom)\n",
    "        x = torchvision.transforms.functional.pad(x, padding, fill=0, padding_mode='constant')\n",
    "    \n",
    "    return x\n",
    "\n",
    "# validation transform - just resize and center crop\n",
    "def val_transform(crop_size):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        Lambda(lambda x: pad_to_size(x, crop_size)),\n",
    "        torchvision.transforms.CenterCrop((crop_size, crop_size))\n",
    "    ])\n",
    "\n",
    "# training transform - all augmentations including shift\n",
    "def train_transform(crop_size, shift):\n",
    "    return torchvision.transforms.Compose([\n",
    "        torchvision.transforms.Lambda(poisson_sampling),\n",
    "        torchvision.transforms.Lambda(cell_shape_aug),\n",
    "        torchvision.transforms.Lambda(env_shape_aug),\n",
    "        torchvision.transforms.ToTensor(),\n",
    "        Lambda(lambda x: pad_to_size(x, crop_size)),\n",
    "        torchvision.transforms.RandomRotation(degrees=(0, 360)),\n",
    "        Lambda(lambda x: ShiftAugmentation(n_size=crop_size, shift_max=shift)(x) if np.random.random() < 0.5 else x),\n",
    "        torchvision.transforms.CenterCrop((crop_size, crop_size)),\n",
    "        torchvision.transforms.RandomHorizontalFlip(p=0.75),\n",
    "        torchvision.transforms.RandomVerticalFlip(p=0.75),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dataset class\n",
    "\n",
    "pytorch dataset to load the images\n",
    "\n",
    "CellCropsDataset was adapted from CellSighter data.py\n",
    "\n",
    "Since our images are already cropped and CellSighter expects both image and mask inputs, we append a mask as the last channel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CellCropsDataset(Dataset):\n",
    "    def __init__(self, crops, mask=False, transform=None):\n",
    "        super().__init__()\n",
    "        self._crops = crops\n",
    "        self._transform = transform\n",
    "        self._mask = mask\n",
    "    \n",
    "    def __len__(self):\n",
    "        return len(self._crops)\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        sample = self._crops[idx].sample(self._mask)\n",
    "        \n",
    "        # stack image and mask together \n",
    "        # image has 4 channels, then all_cells_mask, then mask\n",
    "        aug = self._transform(np.dstack(\n",
    "            [sample['image'], sample['all_cells_mask'][:, :, np.newaxis], sample['mask'][:, :, np.newaxis]]\n",
    "        )).float()\n",
    "        \n",
    "        # split back to image and mask\n",
    "        sample['image'] = aug[:4, :, :]  # first 4 channels\n",
    "        sample['mask'] = aug[[5], :, :]  # last channel\n",
    "        \n",
    "        # remove fields that cant be batched\n",
    "        if 'all_cells_mask' in sample:\n",
    "            del sample['all_cells_mask']\n",
    "        if 'all_cells_mask_seperate' in sample:\n",
    "            del sample['all_cells_mask_seperate']\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model\n",
    "\n",
    "using resnet50 as backbone, need to change first layer because we have 5 channels (4 image channels + 1 mask)\n",
    "\n",
    "adapted from model.py from CellSighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "class Model(nn.Module):\n",
    "    def __init__(self, input_len, num_classes):\n",
    "        super(Model, self).__init__()\n",
    "        # use resnet50\n",
    "        self.model = models.resnet50(num_classes=num_classes)\n",
    "        \n",
    "        # change first conv layer to accept our number of input channels\n",
    "        self.model.conv1 = torch.nn.Conv2d(input_len, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        \n",
    "        # initialize weights\n",
    "        nn.init.kaiming_normal_(self.model.conv1.weight, mode='fan_out', nonlinearity='relu')\n",
    "        \n",
    "        self.softmax = nn.Softmax(dim=1)\n",
    "    \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        # only apply softmax during inference\n",
    "        if not self.training:\n",
    "            x = self.softmax(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## weighted sampler for class imbalance\n",
    "\n",
    "Some classes have fewer samples, so we apply class weighting to balance the training process\n",
    "\n",
    "adapted from train.py in the CellSighter repository\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def make_weighted_sampler(crops):\n",
    "    labels = np.array([c._label for c in crops])\n",
    "    unique_labels = np.unique(labels)\n",
    "    \n",
    "    # count how many samples in each class\n",
    "    class_counts = {}\n",
    "    for label in unique_labels:\n",
    "        class_counts[label] = len(np.where(labels == label)[0])\n",
    "    \n",
    "    print(\"class distribution:\")\n",
    "    for label, count in class_counts.items():\n",
    "        print(f\"  class {label}: {count} samples\")\n",
    "    \n",
    "    # weight = total / class_count\n",
    "    # rare classes get higher weight\n",
    "    total = sum(class_counts.values())\n",
    "    weights = {}\n",
    "    for label, count in class_counts.items():\n",
    "        weights[label] = total / count\n",
    "    \n",
    "    # assign weight to each sample\n",
    "    sample_weights = np.array([weights[label] for label in labels])\n",
    "    sample_weights = torch.from_numpy(sample_weights)\n",
    "    \n",
    "    return WeightedRandomSampler(sample_weights.double(), len(sample_weights))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training function\n",
    "\n",
    "train for one epoch. also adapted from train.py from CellSighter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_one_epoch(model, dataloader, optimizer, criterion, epoch, device):\n",
    "    model.train()\n",
    "    \n",
    "    total_loss = 0\n",
    "    for i, batch in enumerate(dataloader):\n",
    "        # get data\n",
    "        x = batch['image']\n",
    "        m = batch.get('mask', None)\n",
    "        \n",
    "        # concat mask to image\n",
    "        if m is not None:\n",
    "            x = torch.cat([x, m], dim=1)\n",
    "        \n",
    "        # move to gpu\n",
    "        x = x.to(device=device)\n",
    "        y = batch['label'].to(device=device)\n",
    "        \n",
    "        # forward pass\n",
    "        optimizer.zero_grad()\n",
    "        y_pred = model(x)\n",
    "        loss = criterion(y_pred, y)\n",
    "        \n",
    "        # backprop\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        \n",
    "        # print loss \n",
    "        if i % 50 == 0:\n",
    "            print(f\"epoch {epoch} batch {i}/{len(dataloader)} loss: {loss.item():.4f}\")\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    return avg_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## validation function\n",
    "\n",
    "evaluate on validation set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def validate(model, dataloader, criterion, device):\n",
    "    model.eval()\n",
    "    \n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    # collect all predictions and labels for CSV\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    all_probs = []\n",
    "    all_cell_ids = []\n",
    "    all_image_ids = []\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        for batch in dataloader:\n",
    "            x = batch['image']\n",
    "            m = batch.get('mask', None)\n",
    "            \n",
    "            if m is not None:\n",
    "                x = torch.cat([x, m], dim=1)\n",
    "            \n",
    "            x = x.to(device=device)\n",
    "            y = batch['label'].to(device=device)\n",
    "            \n",
    "            y_pred = model(x)\n",
    "            loss = criterion(y_pred, y)\n",
    "            \n",
    "            total_loss += loss.item()\n",
    "            \n",
    "            # calculate accuracy\n",
    "            _, predicted = torch.max(y_pred, 1)\n",
    "            total += y.size(0)\n",
    "            correct += (predicted == y).sum().item()\n",
    "            \n",
    "            # collect for CSV\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(y.cpu().numpy())\n",
    "            all_probs.append(y_pred.cpu().numpy())\n",
    "            all_cell_ids.extend(batch['cell_id'])\n",
    "            all_image_ids.extend(batch['image_id'])\n",
    "    \n",
    "    avg_loss = total_loss / len(dataloader)\n",
    "    accuracy = correct / total\n",
    "    \n",
    "    # combine all probabilities\n",
    "    all_probs = np.concatenate(all_probs, axis=0)\n",
    "    \n",
    "    return avg_loss, accuracy, all_preds, all_labels, all_probs, all_cell_ids, all_image_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# function to save validation results to CSV\n",
    "def save_val_results(save_path, preds, labels, probs, cell_ids, image_ids):\n",
    "    import pandas as pd\n",
    "    \n",
    "    # create dataframe\n",
    "    results = pd.DataFrame({\n",
    "        'pred': preds,\n",
    "        'label': labels,\n",
    "        'pred_prob': probs.max(axis=1),  # max probability\n",
    "        'cell_id': cell_ids,\n",
    "        'image_id': image_ids,\n",
    "        'prob_list': [prob.tolist() for prob in probs]  # all class probabilities\n",
    "    })\n",
    "    \n",
    "    # save to CSV\n",
    "    results.to_csv(save_path, index=False)\n",
    "    print(f\"saved validation results to: {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded 1882 training samples\n",
      "loaded 467 validation samples\n",
      "class distribution:\n",
      "  class 0: 196 samples\n",
      "  class 1: 75 samples\n",
      "  class 2: 432 samples\n",
      "  class 3: 60 samples\n",
      "  class 4: 61 samples\n",
      "  class 5: 651 samples\n",
      "  class 6: 57 samples\n",
      "  class 7: 350 samples\n"
     ]
    }
   ],
   "source": [
    "# paths\n",
    "train_json = os.path.join(config['root_dir'], 'train.json')\n",
    "val_json = os.path.join(config['root_dir'], 'val.json')\n",
    "images_dir = '/scratch/groups/emmalu/samutiti/raw_phage_crops'\n",
    "\n",
    "train_crops = load_crops(train_json, images_dir)\n",
    "print(f\"loaded {len(train_crops)} training samples\")\n",
    "\n",
    "val_crops = load_crops(val_json, images_dir)\n",
    "print(f\"loaded {len(val_crops)} validation samples\")\n",
    "\n",
    "# make weighted sampler\n",
    "sampler = make_weighted_sampler(train_crops)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "crop size: 60x60\n"
     ]
    }
   ],
   "source": [
    "crop_size = config[\"crop_input_size\"]\n",
    "use_augmentation = config.get(\"aug\", True)\n",
    "\n",
    "# training dataset with augmentation\n",
    "train_transforms = train_transform(crop_size, shift=5) if use_augmentation else val_transform(crop_size)\n",
    "train_dataset = CellCropsDataset(train_crops, transform=train_transforms, mask=True)\n",
    "\n",
    "# validation dataset without augmentation\n",
    "val_dataset = CellCropsDataset(val_crops, transform=val_transform(crop_size), mask=True)\n",
    "\n",
    "print(f\"crop size: {crop_size}x{crop_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## create data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "batch size: 32\n",
      "train batches: 59\n",
      "val batches: 15\n"
     ]
    }
   ],
   "source": [
    "batch_size = config[\"batch_size\"]\n",
    "num_workers = config[\"num_workers\"]\n",
    "use_sampler = config[\"sample_batch\"]\n",
    "\n",
    "train_loader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    sampler=sampler if use_sampler else None,\n",
    "    shuffle=False if use_sampler else True\n",
    ")\n",
    "\n",
    "val_loader = DataLoader(\n",
    "    val_dataset,\n",
    "    batch_size=batch_size,\n",
    "    num_workers=num_workers,\n",
    "    shuffle=False\n",
    ")\n",
    "\n",
    "print(f\"batch size: {batch_size}\")\n",
    "print(f\"train batches: {len(train_loader)}\")\n",
    "print(f\"val batches: {len(val_loader)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup model\n",
    "\n",
    "create the model and move it to gpu"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "using device: cuda\n"
     ]
    }
   ],
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"using device: {device}\")\n",
    "\n",
    "# 4 image channels + 1 mask channel = 5 input channels\n",
    "num_input_channels = 5\n",
    "num_classes = config[\"num_classes\"]\n",
    "\n",
    "# create model\n",
    "model = Model(num_input_channels, num_classes)\n",
    "model = model.to(device=device)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## setup optimizer and loss\n",
    "\n",
    "using adam optimizer and cross entropy loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning rate: 0.001\n",
      "number of epochs: 50\n"
     ]
    }
   ],
   "source": [
    "learning_rate = config[\"lr\"]\n",
    "num_epochs = config[\"epoch_max\"]\n",
    "\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)\n",
    "scheduler = torch.optim.lr_scheduler.ExponentialLR(optimizer, gamma=0.85)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "print(f\"learning rate: {learning_rate}\")\n",
    "print(f\"number of epochs: {num_epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## training loop\n",
    "\n",
    "now train the model! takes about 1.5 hr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting training...\n",
      "\n",
      "\n",
      "===== Epoch 1/50 =====\n",
      "epoch 0 batch 0/59 loss: 2.2552\n",
      "epoch 0 batch 50/59 loss: 1.7576\n",
      "training loss: 2.3620\n",
      "validation loss: 2.0000\n",
      "validation accuracy: 0.3405\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_0.csv\n",
      "saved checkpoint: /scratch/users/rchi/cs230/macrophage_experiment/checkpoint_epoch_0.pth\n",
      "\n",
      "===== Epoch 2/50 =====\n",
      "epoch 1 batch 0/59 loss: 1.8510\n",
      "epoch 1 batch 50/59 loss: 2.0272\n",
      "training loss: 1.8098\n",
      "\n",
      "===== Epoch 3/50 =====\n",
      "epoch 2 batch 0/59 loss: 1.8455\n",
      "epoch 2 batch 50/59 loss: 1.7452\n",
      "training loss: 1.8475\n",
      "\n",
      "===== Epoch 4/50 =====\n",
      "epoch 3 batch 0/59 loss: 1.6943\n",
      "epoch 3 batch 50/59 loss: 1.5884\n",
      "training loss: 1.7351\n",
      "\n",
      "===== Epoch 5/50 =====\n",
      "epoch 4 batch 0/59 loss: 1.5499\n",
      "epoch 4 batch 50/59 loss: 1.7386\n",
      "training loss: 1.6393\n",
      "\n",
      "===== Epoch 6/50 =====\n",
      "epoch 5 batch 0/59 loss: 1.4242\n",
      "epoch 5 batch 50/59 loss: 1.6248\n",
      "training loss: 1.6309\n",
      "validation loss: 1.9666\n",
      "validation accuracy: 0.2955\n",
      "\n",
      "===== Epoch 7/50 =====\n",
      "epoch 6 batch 0/59 loss: 1.4328\n",
      "epoch 6 batch 50/59 loss: 1.5160\n",
      "training loss: 1.5254\n",
      "\n",
      "===== Epoch 8/50 =====\n",
      "epoch 7 batch 0/59 loss: 1.3007\n",
      "epoch 7 batch 50/59 loss: 1.2185\n",
      "training loss: 1.5416\n",
      "\n",
      "===== Epoch 9/50 =====\n",
      "epoch 8 batch 0/59 loss: 1.8646\n",
      "epoch 8 batch 50/59 loss: 1.3691\n",
      "training loss: 1.4884\n",
      "\n",
      "===== Epoch 10/50 =====\n",
      "epoch 9 batch 0/59 loss: 1.5365\n",
      "epoch 9 batch 50/59 loss: 1.5436\n",
      "training loss: 1.3835\n",
      "\n",
      "===== Epoch 11/50 =====\n",
      "epoch 10 batch 0/59 loss: 1.2895\n",
      "epoch 10 batch 50/59 loss: 1.5573\n",
      "training loss: 1.3639\n",
      "validation loss: 1.9418\n",
      "validation accuracy: 0.3533\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_10.csv\n",
      "saved checkpoint: /scratch/users/rchi/cs230/macrophage_experiment/checkpoint_epoch_10.pth\n",
      "learning rate updated to: 0.000850\n",
      "\n",
      "===== Epoch 12/50 =====\n",
      "epoch 11 batch 0/59 loss: 1.0106\n",
      "epoch 11 batch 50/59 loss: 1.5527\n",
      "training loss: 1.4605\n",
      "\n",
      "===== Epoch 13/50 =====\n",
      "epoch 12 batch 0/59 loss: 1.7226\n",
      "epoch 12 batch 50/59 loss: 1.3168\n",
      "training loss: 1.3927\n",
      "\n",
      "===== Epoch 14/50 =====\n",
      "epoch 13 batch 0/59 loss: 1.2958\n",
      "epoch 13 batch 50/59 loss: 1.0716\n",
      "training loss: 1.3588\n",
      "\n",
      "===== Epoch 15/50 =====\n",
      "epoch 14 batch 0/59 loss: 1.3088\n",
      "epoch 14 batch 50/59 loss: 1.3914\n",
      "training loss: 1.3081\n",
      "\n",
      "===== Epoch 16/50 =====\n",
      "epoch 15 batch 0/59 loss: 1.5286\n",
      "epoch 15 batch 50/59 loss: 1.2627\n",
      "training loss: 1.2893\n",
      "validation loss: 1.9339\n",
      "validation accuracy: 0.3212\n",
      "\n",
      "===== Epoch 17/50 =====\n",
      "epoch 16 batch 0/59 loss: 1.0921\n",
      "epoch 16 batch 50/59 loss: 1.3222\n",
      "training loss: 1.2542\n",
      "\n",
      "===== Epoch 18/50 =====\n",
      "epoch 17 batch 0/59 loss: 1.2144\n",
      "epoch 17 batch 50/59 loss: 0.9675\n",
      "training loss: 1.2174\n",
      "\n",
      "===== Epoch 19/50 =====\n",
      "epoch 18 batch 0/59 loss: 1.4233\n",
      "epoch 18 batch 50/59 loss: 1.1639\n",
      "training loss: 1.2023\n",
      "\n",
      "===== Epoch 20/50 =====\n",
      "epoch 19 batch 0/59 loss: 1.3767\n",
      "epoch 19 batch 50/59 loss: 0.8200\n",
      "training loss: 1.1280\n",
      "\n",
      "===== Epoch 21/50 =====\n",
      "epoch 20 batch 0/59 loss: 0.8569\n",
      "epoch 20 batch 50/59 loss: 1.5815\n",
      "training loss: 1.2113\n",
      "validation loss: 1.9234\n",
      "validation accuracy: 0.3576\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_20.csv\n",
      "saved checkpoint: /scratch/users/rchi/cs230/macrophage_experiment/checkpoint_epoch_20.pth\n",
      "learning rate updated to: 0.000722\n",
      "\n",
      "===== Epoch 22/50 =====\n",
      "epoch 21 batch 0/59 loss: 1.5399\n",
      "epoch 21 batch 50/59 loss: 1.0179\n",
      "training loss: 1.0549\n",
      "\n",
      "===== Epoch 23/50 =====\n",
      "epoch 22 batch 0/59 loss: 0.9447\n",
      "epoch 22 batch 50/59 loss: 1.6198\n",
      "training loss: 1.0660\n",
      "\n",
      "===== Epoch 24/50 =====\n",
      "epoch 23 batch 0/59 loss: 0.9274\n",
      "epoch 23 batch 50/59 loss: 1.1009\n",
      "training loss: 1.0249\n",
      "\n",
      "===== Epoch 25/50 =====\n",
      "epoch 24 batch 0/59 loss: 1.0616\n",
      "epoch 24 batch 50/59 loss: 0.9464\n",
      "training loss: 1.0854\n",
      "\n",
      "===== Epoch 26/50 =====\n",
      "epoch 25 batch 0/59 loss: 1.0084\n",
      "epoch 25 batch 50/59 loss: 1.3939\n",
      "training loss: 1.0263\n",
      "validation loss: 1.8745\n",
      "validation accuracy: 0.4283\n",
      "\n",
      "===== Epoch 27/50 =====\n",
      "epoch 26 batch 0/59 loss: 0.9604\n",
      "epoch 26 batch 50/59 loss: 0.9807\n",
      "training loss: 1.0158\n",
      "\n",
      "===== Epoch 28/50 =====\n",
      "epoch 27 batch 0/59 loss: 0.7716\n",
      "epoch 27 batch 50/59 loss: 1.0983\n",
      "training loss: 0.9780\n",
      "\n",
      "===== Epoch 29/50 =====\n",
      "epoch 28 batch 0/59 loss: 0.8733\n",
      "epoch 28 batch 50/59 loss: 0.7380\n",
      "training loss: 0.9125\n",
      "\n",
      "===== Epoch 30/50 =====\n",
      "epoch 29 batch 0/59 loss: 0.6999\n",
      "epoch 29 batch 50/59 loss: 1.0846\n",
      "training loss: 0.9592\n",
      "\n",
      "===== Epoch 31/50 =====\n",
      "epoch 30 batch 0/59 loss: 0.9879\n",
      "epoch 30 batch 50/59 loss: 1.0040\n",
      "training loss: 0.9449\n",
      "validation loss: 1.8556\n",
      "validation accuracy: 0.4347\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_30.csv\n",
      "saved checkpoint: /scratch/users/rchi/cs230/macrophage_experiment/checkpoint_epoch_30.pth\n",
      "learning rate updated to: 0.000614\n",
      "\n",
      "===== Epoch 32/50 =====\n",
      "epoch 31 batch 0/59 loss: 1.0163\n",
      "epoch 31 batch 50/59 loss: 1.2125\n",
      "training loss: 0.9252\n",
      "\n",
      "===== Epoch 33/50 =====\n",
      "epoch 32 batch 0/59 loss: 0.7827\n",
      "epoch 32 batch 50/59 loss: 1.2052\n",
      "training loss: 0.8970\n",
      "\n",
      "===== Epoch 34/50 =====\n",
      "epoch 33 batch 0/59 loss: 0.8950\n",
      "epoch 33 batch 50/59 loss: 0.8093\n",
      "training loss: 0.8434\n",
      "\n",
      "===== Epoch 35/50 =====\n",
      "epoch 34 batch 0/59 loss: 0.8275\n",
      "epoch 34 batch 50/59 loss: 0.6129\n",
      "training loss: 0.8408\n",
      "\n",
      "===== Epoch 36/50 =====\n",
      "epoch 35 batch 0/59 loss: 1.0815\n",
      "epoch 35 batch 50/59 loss: 0.6646\n",
      "training loss: 0.8449\n",
      "validation loss: 1.8301\n",
      "validation accuracy: 0.4690\n",
      "\n",
      "===== Epoch 37/50 =====\n",
      "epoch 36 batch 0/59 loss: 0.6710\n",
      "epoch 36 batch 50/59 loss: 0.7412\n",
      "training loss: 0.7949\n",
      "\n",
      "===== Epoch 38/50 =====\n",
      "epoch 37 batch 0/59 loss: 0.7702\n",
      "epoch 37 batch 50/59 loss: 0.9817\n",
      "training loss: 0.8631\n",
      "\n",
      "===== Epoch 39/50 =====\n",
      "epoch 38 batch 0/59 loss: 0.6547\n",
      "epoch 38 batch 50/59 loss: 1.1079\n",
      "training loss: 0.8217\n",
      "\n",
      "===== Epoch 40/50 =====\n",
      "epoch 39 batch 0/59 loss: 0.6632\n",
      "epoch 39 batch 50/59 loss: 1.2860\n",
      "training loss: 0.7625\n",
      "\n",
      "===== Epoch 41/50 =====\n",
      "epoch 40 batch 0/59 loss: 0.7218\n",
      "epoch 40 batch 50/59 loss: 1.2046\n",
      "training loss: 0.8017\n",
      "validation loss: 1.8155\n",
      "validation accuracy: 0.5139\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_40.csv\n",
      "saved checkpoint: /scratch/users/rchi/cs230/macrophage_experiment/checkpoint_epoch_40.pth\n",
      "learning rate updated to: 0.000522\n",
      "\n",
      "===== Epoch 42/50 =====\n",
      "epoch 41 batch 0/59 loss: 0.4896\n",
      "epoch 41 batch 50/59 loss: 0.8121\n",
      "training loss: 0.7472\n",
      "\n",
      "===== Epoch 43/50 =====\n",
      "epoch 42 batch 0/59 loss: 0.6150\n",
      "epoch 42 batch 50/59 loss: 0.3729\n",
      "training loss: 0.7122\n",
      "\n",
      "===== Epoch 44/50 =====\n",
      "epoch 43 batch 0/59 loss: 0.6415\n",
      "epoch 43 batch 50/59 loss: 0.5144\n",
      "training loss: 0.6538\n",
      "\n",
      "===== Epoch 45/50 =====\n",
      "epoch 44 batch 0/59 loss: 0.5260\n",
      "epoch 44 batch 50/59 loss: 0.8795\n",
      "training loss: 0.6732\n",
      "\n",
      "===== Epoch 46/50 =====\n",
      "epoch 45 batch 0/59 loss: 0.5560\n",
      "epoch 45 batch 50/59 loss: 0.5799\n",
      "training loss: 0.6306\n",
      "validation loss: 1.8017\n",
      "validation accuracy: 0.4946\n",
      "\n",
      "===== Epoch 47/50 =====\n",
      "epoch 46 batch 0/59 loss: 0.4596\n",
      "epoch 46 batch 50/59 loss: 0.6439\n",
      "training loss: 0.6288\n",
      "\n",
      "===== Epoch 48/50 =====\n",
      "epoch 47 batch 0/59 loss: 0.4699\n",
      "epoch 47 batch 50/59 loss: 0.7221\n",
      "training loss: 0.6661\n",
      "\n",
      "===== Epoch 49/50 =====\n",
      "epoch 48 batch 0/59 loss: 0.5144\n",
      "epoch 48 batch 50/59 loss: 0.5084\n",
      "training loss: 0.6506\n",
      "\n",
      "===== Epoch 50/50 =====\n",
      "epoch 49 batch 0/59 loss: 0.3963\n",
      "epoch 49 batch 50/59 loss: 0.5592\n",
      "training loss: 0.6050\n",
      "\n",
      "training finished!\n"
     ]
    }
   ],
   "source": [
    "# where to save checkpoints\n",
    "save_dir = '/scratch/users/rchi/cs230/macrophage_experiment'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "\n",
    "# keep track of losses\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "val_accuracies = []\n",
    "\n",
    "print(\"starting training...\\n\")\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    print(f\"\\n===== Epoch {epoch+1}/{num_epochs} =====\")\n",
    "    \n",
    "    # train\n",
    "    train_loss = train_one_epoch(model, train_loader, optimizer, criterion, epoch, device)\n",
    "    train_losses.append(train_loss)\n",
    "    print(f\"training loss: {train_loss:.4f}\")\n",
    "    \n",
    "    # validate every 5 epochs\n",
    "    if epoch % 5 == 0:\n",
    "        val_loss, val_acc, preds, labels, probs, cell_ids, image_ids = validate(model, val_loader, criterion, device)\n",
    "        val_losses.append(val_loss)\n",
    "        val_accuracies.append(val_acc)\n",
    "        print(f\"validation loss: {val_loss:.4f}\")\n",
    "        print(f\"validation accuracy: {val_acc:.4f}\")\n",
    "        \n",
    "        # save validation CSV every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            val_csv_path = os.path.join(save_dir, f\"val_results_{epoch}.csv\")\n",
    "            save_val_results(val_csv_path, preds, labels, probs, cell_ids, image_ids)\n",
    "    \n",
    "    # save checkpoint every 10 epochs\n",
    "    if epoch % 10 == 0:\n",
    "        checkpoint_path = os.path.join(save_dir, f\"checkpoint_epoch_{epoch}.pth\")\n",
    "        torch.save({\n",
    "            'epoch': epoch,\n",
    "            'model_state_dict': model.state_dict(),\n",
    "            'optimizer_state_dict': optimizer.state_dict(),\n",
    "            'train_loss': train_loss,\n",
    "        }, checkpoint_path)\n",
    "        print(f\"saved checkpoint: {checkpoint_path}\")\n",
    "    \n",
    "    # update learning rate\n",
    "    if epoch > 0 and epoch % 10 == 0:\n",
    "        scheduler.step()\n",
    "        current_lr = optimizer.param_groups[0]['lr']\n",
    "        print(f\"learning rate updated to: {current_lr:.6f}\")\n",
    "\n",
    "print(\"\\ntraining finished!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## final evaluation\n",
    "\n",
    "test on validation set one more time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "running final evaluation...\n",
      "\n",
      "final validation loss: 1.7902\n",
      "final validation accuracy: 0.5054\n",
      "\n",
      "saved final model to: /scratch/users/rchi/cs230/macrophage_experiment/final_model.pth\n",
      "saved validation results to: /scratch/users/rchi/cs230/macrophage_experiment/val_results_final.csv\n"
     ]
    }
   ],
   "source": [
    "print(\"running final evaluation...\")\n",
    "val_loss, val_acc, preds, labels, probs, cell_ids, image_ids = validate(model, val_loader, criterion, device)\n",
    "\n",
    "print(f\"\\nfinal validation loss: {val_loss:.4f}\")\n",
    "print(f\"final validation accuracy: {val_acc:.4f}\")\n",
    "\n",
    "# save final model\n",
    "final_model_path = os.path.join(save_dir, \"final_model.pth\")\n",
    "torch.save(model.state_dict(), final_model_path)\n",
    "print(f\"\\nsaved final model to: {final_model_path}\")\n",
    "\n",
    "# save final validation results CSV\n",
    "final_csv_path = os.path.join(save_dir, \"val_results_final.csv\")\n",
    "save_val_results(final_csv_path, preds, labels, probs, cell_ids, image_ids)\n",
    "\n",
    "# store for later use\n",
    "final_val_loss = val_loss\n",
    "final_val_acc = val_acc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## plot training curves\n",
    "\n",
    "visualize the training progress"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## confusion matrix\n",
    "\n",
    "lets see which classes get confused with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'plt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[23], line 32\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;66;03m# plot it\u001b[39;00m\n\u001b[1;32m     30\u001b[0m class_names \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mDividing\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mEarly Phago\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mFried egg\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mInter Phago\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLate Phago\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMigrating\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mQuiescent\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mSearching\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[0;32m---> 32\u001b[0m \u001b[43mplt\u001b[49m\u001b[38;5;241m.\u001b[39mfigure(figsize\u001b[38;5;241m=\u001b[39m(\u001b[38;5;241m10\u001b[39m, \u001b[38;5;241m8\u001b[39m))\n\u001b[1;32m     33\u001b[0m sns\u001b[38;5;241m.\u001b[39mheatmap(cm, annot\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, fmt\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124md\u001b[39m\u001b[38;5;124m'\u001b[39m, cmap\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mBlues\u001b[39m\u001b[38;5;124m'\u001b[39m, xticklabels\u001b[38;5;241m=\u001b[39mclass_names, yticklabels\u001b[38;5;241m=\u001b[39mclass_names)\n\u001b[1;32m     34\u001b[0m plt\u001b[38;5;241m.\u001b[39mxlabel(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpredicted\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'plt' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "# get predictions for all validation data\n",
    "model.eval()\n",
    "all_preds = []\n",
    "all_labels = []\n",
    "\n",
    "with torch.no_grad():\n",
    "    for batch in val_loader:\n",
    "        x = batch['image']\n",
    "        m = batch.get('mask', None)\n",
    "        \n",
    "        if m is not None:\n",
    "            x = torch.cat([x, m], dim=1)\n",
    "        \n",
    "        x = x.to(device=device)\n",
    "        y = batch['label'].to(device=device)\n",
    "        \n",
    "        y_pred = model(x)\n",
    "        _, predicted = torch.max(y_pred, 1)\n",
    "        \n",
    "        all_preds.extend(predicted.cpu().numpy())\n",
    "        all_labels.extend(y.cpu().numpy())\n",
    "\n",
    "# make confusion matrix\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "\n",
    "# plot it\n",
    "class_names = ['Dividing', 'Early Phago', 'Fried egg', 'Inter Phago', 'Late Phago', 'Migrating', 'Quiescent', 'Searching']\n",
    "\n",
    "plt.figure(figsize=(10, 8))\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('predicted')\n",
    "plt.ylabel('true')\n",
    "plt.title('confusion matrix')\n",
    "plt.tight_layout()\n",
    "plt.savefig(os.path.join(save_dir, 'confusion_matrix.png'))\n",
    "plt.show()\n",
    "\n",
    "print(\"confusion matrix saved\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## per class accuracy\n",
    "\n",
    "see how well each class does"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5 (cs230)",
   "language": "python",
   "name": "cs230"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
